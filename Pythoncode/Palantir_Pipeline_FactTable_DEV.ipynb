{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deciphering the generated Fact table:\n",
        "\n",
        "\n",
        "*  This includes concept sets that are automatically added to all patient datasets.\n",
        "\n",
        "These concept sets were created by other team members and are based on clinical diagnoses that generally align with overall diagnostic criteria.\n"
      ],
      "metadata": {
        "id": "5cHohfhVwGPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to isolate and analyze records pertaining exclusively to delivery patients."
      ],
      "metadata": {
        "id": "9VxPKH_bwGeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: The purpose of this table is to create transform of the all_patients_fact_day_table_de_id_ We are going to add patients that delivered and its 1 and pregnancy is 1. This is to narrow the cohort.\n",
        "# Creator: for the Team1 project\n",
        "# Created/Updated 04/01/2025\n",
        "\n",
        "# Description: Used delivery records only.\n",
        "# - This code does not include pregnancies that involved abortions or complications. The cohorts are strictly categorized with a value of 1 for both delivery mode and pregnancy.\n",
        "\n",
        "# - The Value 0 means Missing data or any event that happend and person didn't deliver. Adding the additional sorting will help us to include only FEMALE and exclude MALE. This can be done at the colpsed table at the end.\n",
        "# - ! The missing data is represented as 0 so filtering dawn on pregnancy as 1 where the pregnancy was recorded at the visit. We will then exclud missing data from the cohort.\n",
        "# - It filters the DataFrame to include only those rows where the column delivery_record_only equals 1. This step isolates patients who have a delivery record.\n",
        "# - Pulling all delivery patients form the data makse more sens adding the information abot number of givenbirths at this moment\n",
        "# combin all filtering on one node that is ithe seaiset wayt o avoid the program bugs, if you want ot filter in the next node you need to recall the first node as dataframe df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def Delivery_index_date_setup_1(all_patients_fact_day_table_de_id):\n",
        "    df = all_patients_fact_day_table_de_id\n",
        "\n",
        "    # Print the original sample size\n",
        "    print(\"Original sample size:\", df.count())\n",
        "\n",
        "    # Filter for rows where delivery_record_only == 1 or number_given_birth == 1\n",
        "    filtered_df = df.filter(\n",
        "    (F.col(\"delivery_record_only\") == 1) |\n",
        "    (F.col(\"number_given_birth\") == 1) |\n",
        "    (F.col(\"pregnancy\") ==1)\n",
        "    )\n",
        "\n",
        "    # Further filter where pregnancy == 1\n",
        "    # filtered_df = filtered_df.filter(F.col(\"pregnancy\") == 1)\n",
        "    # Print the filtered sample size\n",
        "    print(\"Filtered sample size:\", filtered_df.count())\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SIrTukUowJvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Follwong code will allow us to filter data further as wee see that pragnancy conceptset gives us 1 or 0. which means 0 there might not be record or missing data that NA is given as 0. Additional filterinif done separetly will not take into account the previous filtering.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MWGD2o0L9SoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: The purpose of this table is to create transform of the all_patients_fact_day_table_de_id_ We are going to add patients that delivered and its 1 and pregnancy is 1. This is to narrow the cohort.\n",
        "# Creator: for the Team1 project\n",
        "# Created/Updated 04/01/2025\n",
        "# Desctiprion: Pregnancy as 1 set for our incusion criteria. This code is implementing pregnancy as 1.\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def Only_Pregnant_as1(Delivery_index_date_setup_1):\n",
        "    df = Delivery_index_date_setup_1\n",
        "\n",
        "    # Print the original sample siz\n",
        "    print(\"Original1 sample size:\", df.count())\n",
        "\n",
        "    # Filter for rows where delivery_record_only == 1\n",
        "    filtered_df = df.filter (F.col(\"pregnancy\") == 1)\n",
        "\n",
        "    # Print the filtered sample size\n",
        "    print(\"Filtered2 sample size:\", filtered_df.count())\n",
        "\n",
        "\n",
        "    return filtered_df\n"
      ],
      "metadata": {
        "id": "3i8HyUQnwJ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up delivery date . Where we are intersted in the first delivery date, simple and less heavy no addition of extra columns"
      ],
      "metadata": {
        "id": "1qyHyVNHAQOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: The purpose of this table is to create transform of the Delivery to Only_Pregnant_as1_\n",
        "# Creator: for the Team1 project\n",
        "# Created/Updated 04/03/2025\n",
        "# Description: As the DElivery Table ony contains the Delivery as 1 and pregnancy as 1. The code dosn't take the other virables to narrow donw to pregnancy. I have created a separate node to narrow down the delivery_1 to Pregnancy_1. I think we should sort first as a  pregnancy and from that cohort narrow down the deliery. However, in pregnancy table 0 means missing data for the explicite date of the visit for the paptient.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "def DDateSetup( Only_Pregnant_as1):\n",
        "    # Filter for delivery records\n",
        "    deliveries = Only_Pregnant_as1.filter(F.col(\"delivery_record_only\") == 1)\n",
        "\n",
        "    # Get the first delivery date per person\n",
        "    first_del = deliveries.groupBy(\"person_id\").agg(\n",
        "        F.min(\"date\").alias(\"first_delivery_date\")\n",
        "    )\n",
        "\n",
        "    print(\"First delivery sample size:\", first_del.count())  # Moved inside the function\n",
        "\n",
        "    return first_del  # Correct indentation\n"
      ],
      "metadata": {
        "id": "dIk6G19pAPQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### addition of the columns"
      ],
      "metadata": {
        "id": "711MSsfgHJrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: The purpose of this table is to merge boh tables that were filtered down and bring the first delivery date into the table.\n",
        "# Creator: for the Team1 project\n",
        "# Created/Updated 04/03/2025\n",
        "# Description:\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def merging(DDateSetup, Only_Pregnant_as1):\n",
        "\n",
        "    merged_df = DDateSetup.join(\n",
        "        Only_Pregnant_as1,\n",
        "        on=\"person_id\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    return merged_df\n"
      ],
      "metadata": {
        "id": "VzWTLDsXAPZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the Delivery flags of 1 year prior, post and 280 days prior as sensitivity test\n"
      ],
      "metadata": {
        "id": "UbUgm-4eHx17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the flag to all patients for the delivery 1 year prior 1 yer post and 280 prior\n",
        "\n",
        "# This will allow us to sort patients that had only these instances delivery pre post and clinical diagnosis of any concepts that we have interst in.\n",
        "#for EHR data it comes as a clustered toogether concepts and can be segregated on this basis. To indentify Delivery date for these columns in our consepts\n",
        "\n",
        "\n",
        "def All_PATIENTS_FLAG(merging):\n",
        "\n",
        "\n",
        "    df = merging\n",
        "\n",
        "# Create delivery 1 year prior and 1 year post dates from first_delivery_date plus additional 280 days prior\n",
        "\n",
        "    df = df.withColumn(\"date_1y_prior_1st_delivery\", F.add_months(F.col(\"first_delivery_date\"), -12))\n",
        "    df = df.withColumn(\"date_1y_after_1st_delivery\", F.add_months(F.col(\"first_delivery_date\"), +12))\n",
        "    df = df.withColumn(\"date_280d_prior_1st_delivery\", F.date_add(F.col(\"first_delivery_date\"), -280))\n",
        "\n",
        "\n",
        "    # To sate delivery date for these columns in our conceptsets that are clustured toogether in the excel (table content) if something is missing willnot be covred in these groups\n",
        "\n",
        "    all_columns = df.columns\n",
        "    start_col = \"delivery_mode_caesarean\"\n",
        "    end_col = \"patient_death_at_visit\"\n",
        "\n",
        "    start_idx = all_columns.index(start_col)\n",
        "    end_idx = all_columns.index(end_col)\n",
        "    dv_columns = all_columns[start_idx:end_idx + 1]\n",
        "\n",
        "\n",
        "    # Add *_flag_1y and *_flag_280d columns per DV\n",
        "\n",
        "    for col_name in dv_columns:\n",
        "\n",
        "        # Flag using 1-year prior\n",
        "        flag_col_1y = f\"{col_name}_flag_1y\"\n",
        "\n",
        "        df = df.withColumn(\n",
        "            flag_col_1y,\n",
        "            F.when(F.col(col_name) == 0, \"no\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") < F.col(\"date_1y_prior_1st_delivery\")),\n",
        "                \"before_1y_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"date_1y_prior_1st_delivery\")) &\n",
        "                (F.col(\"date\") < F.col(\"first_delivery_date\")),\n",
        "                \"during_1y_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"first_delivery_date\")) &\n",
        "                (F.col(\"date\") <= F.col(\"date_1y_after_1st_delivery\")),\n",
        "                \"during_1y_postpartum\")\n",
        "             .otherwise(\"undefined\")\n",
        "        )\n",
        "\n",
        "        # Flag using 280-day prior\n",
        "        flag_col_280d = f\"{col_name}_flag_280d\"\n",
        "\n",
        "        df = df.withColumn(\n",
        "            flag_col_280d,\n",
        "            F.when(F.col(col_name) == 0, \"no\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") < F.col(\"date_280d_prior_1st_delivery\")),\n",
        "                \"before_280d_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"date_280d_prior_1st_delivery\")) &\n",
        "                (F.col(\"date\") < F.col(\"first_delivery_date\")),\n",
        "                \"during_280d_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"first_delivery_date\")) &\n",
        "                (F.col(\"date\") < F.col(\"date_1y_after_1st_delivery\")),\n",
        "                \"during_1y_postpartum\")\n",
        "             .otherwise(\"undefined\")\n",
        "        )\n",
        "\n",
        "\n",
        "    # Create dummy variables for all *_flag columns\n",
        "\n",
        "    flag_cols = [col for col in df.columns if col.endswith(\"_flag_1y\") or col.endswith(\"_flag_280d\")]\n",
        "\n",
        "    for flag_col in flag_cols:\n",
        "        unique_values = df.select(flag_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "        for value in unique_values:\n",
        "            if value is not None:\n",
        "                safe_value = value.replace(\" \", \"_\")\n",
        "                dummy_col_name = f\"{flag_col}_{safe_value}\"\n",
        "                df = df.withColumn(\n",
        "                    dummy_col_name,\n",
        "                    F.when(F.col(flag_col) == value, 1).otherwise(0)\n",
        "                )\n",
        "\n",
        "\n",
        "    # Drop dummy columns for \"no\" and \"undefined\"\n",
        "\n",
        "    drop_cols = [col for col in df.columns if col.endswith(\"_flag_1y_no\") or col.endswith(\"_flag_1y_undefined\") or col.endswith(\"_flag_280d_no\") or col.endswith(\"_flag_280d_undefined\")]\n",
        "    df = df.drop(*drop_cols)\n",
        "\n",
        "\n",
        "    # Return final DataFrame with flags and dummies\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Global imports and functions included below\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqU6o21XH5ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#additional changes in the code\n",
        "# Adding the flag to all patients for the delivery 1 year prior 1 yer post and 280 prior\n",
        "\n",
        "# This will allow us to sort patients that had only these instances delivery pre post and clinical diagnosis of any concepts that we have interst in.\n",
        "#for EHR data it comes as a clustered toogether concepts and can be segregated on this basis. To indentify Delivery date for these columns in our consepts\n",
        "# Taging the columns uing year prior and 280 days prior\n",
        "# Preparing data for the model using onehot encoding for yes or no 1 and 0\n",
        "\n",
        "\n",
        "def All_PATIENTS_FLAG(merging):\n",
        "\n",
        "\n",
        "    df = merging\n",
        "\n",
        "# Create delivery 1 year prior and 1 year post dates from first_delivery_date plus additional 280 days prior\n",
        "\n",
        "    df = df.withColumn(\"_1y_prior_1st_delivery\", F.add_months(F.col(\"first_delivery_date\"), -12))\n",
        "    df = df.withColumn(\"_1y_after_1st_delivery\", F.add_months(F.col(\"first_delivery_date\"), +12))\n",
        "    df = df.withColumn(\"_280d_prior_1st_delivery\", F.date_add(F.col(\"first_delivery_date\"), -280))\n",
        "# To sate delivery date for these columns in our conceptsets that are clustured toogether in the excel (table content) if something is missing willnot be covred in these groups\n",
        "    all_columns = df.columns\n",
        "    start_col = \"delivery_mode_caesarean\"\n",
        "    end_col = \"patient_death_at_visit\"\n",
        "\n",
        "    start_idx = all_columns.index(start_col)\n",
        "    end_idx = all_columns.index(end_col)\n",
        "    delv_columns = all_columns[start_idx:end_idx + 1]\n",
        "\n",
        "\n",
        "# Add *_flag_1y and *_flag_280d columns per Delivery\n",
        "\n",
        "    for col_name in delv_columns:\n",
        "\n",
        "# Flag using 1-year prior\n",
        "        flag_col_1y = f\"{col_name}_flag_1y\"\n",
        "\n",
        "        df = df.withColumn(\n",
        "            flag_col_1y,\n",
        "            F.when(F.col(col_name) == 0, \"no\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") < F.col(\"_1y_prior_1st_delivery\")),\n",
        "                \"before_1y_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"_1y_prior_1st_delivery\")) &\n",
        "                (F.col(\"date\") < F.col(\"first_delivery_date\")),\n",
        "                \"during_1y_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"first_delivery_date\")) &\n",
        "                (F.col(\"date\") <= F.col(\"_1y_after_1st_delivery\")),\n",
        "                \"during_1y_postpartum\")\n",
        "             .otherwise(\"undefined\")\n",
        "        )\n",
        "# Flag using 280-day prior\n",
        "        flag_col_280d = f\"{col_name}_flag_280d\"\n",
        "\n",
        "        df = df.withColumn(\n",
        "            flag_col_280d,\n",
        "            F.when(F.col(col_name) == 0, \"no\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") < F.col(\"_280d_prior_1st_delivery\")),\n",
        "                \"before_280d_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"_280d_prior_1st_delivery\")) &\n",
        "                (F.col(\"date\") < F.col(\"first_delivery_date\")),\n",
        "                \"during_280d_prior_to_1st_delivery\")\n",
        "             .when(\n",
        "                (F.col(col_name) == 1) &\n",
        "                (F.col(\"date\") >= F.col(\"first_delivery_date\")) &\n",
        "                (F.col(\"date\") < F.col(\"_1y_after_1st_delivery\")),\n",
        "                \"during_1y_postpartum\")\n",
        "             .otherwise(\"undefined\")\n",
        "        )\n",
        "# Create one hot encoding variables for all *_flag columns\n",
        "\n",
        "    flag_cols = [col for col in df.columns if col.endswith(\"_flag_1y\") or col.endswith(\"_flag_280d\")]\n",
        "\n",
        "    for flag_col in flag_cols:\n",
        "        unique_values = df.select(flag_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "        for value in unique_values:\n",
        "            if value is not None:\n",
        "                safe_value = value.replace(\" \", \"_\")\n",
        "                dummy_col_name = f\"{flag_col}_{safe_value}\"\n",
        "                df = df.withColumn(\n",
        "                    dummy_col_name,\n",
        "                    F.when(F.col(flag_col) == value, 1).otherwise(0)\n",
        "                )\n",
        "\n",
        "\n",
        "# Drop one hot encoding columns for \"no\" and \"undefined\"\n",
        "\n",
        "    drop_cols = [col for col in df.columns if col.endswith(\"_flag_1y_no\") or col.endswith(\"_flag_1y_undefined\") or col.endswith(\"_flag_280d_no\") or col.endswith(\"_flag_280d_undefined\")]\n",
        "    df = df.drop(*drop_cols)\n",
        "\n",
        "\n",
        "# Return final DataFrame with flags and dummies\n",
        "\n",
        "    return df\n",
        "\n",
        "#Global imports and functions included below\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wG9DtljdCtZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuing the code with adding another one hot encodign into the variables"
      ],
      "metadata": {
        "id": "FRfEyi-7Gndi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: This function ALL_PATIENTS_SUMMARY performs aggregation and summarization of one-hot encoded patient features from a Spark DataFrame for the Team1 project.\n",
        "# It processes binary event flags (*_flag_1y_*, *_flag_280d_*), computes summary indicators, and joins with demographic and death data for adult female patients.\n",
        "# Creator: for Team1 project\n",
        "\n",
        "# description: Step-by-step Summary: #1. Input DataFrames:\n",
        "\n",
        "# All_PATIENTS_FLAG: Contains one-hot encoded clinical/event flags.\n",
        "\n",
        "# everyone_cohort_de_id: Base cohort with patient demographics.\n",
        "\n",
        "# person: Unused in this function.\n",
        "\n",
        "# everyone_patient_deaths: Contains death status per patient.\n",
        "\n",
        "## 2. Group One-Hot Encoded Columns:\n",
        "\n",
        "# Uses _flag_1y_ and _flag_280d_ patterns to group similar events.\n",
        "\n",
        "# 3. Aggregate Flags:\n",
        "\n",
        "# Converts each group of flags into a single summary column (e.g., *_indicator) using max() to indicate whether the event occurred.\n",
        "\n",
        "# 4. Add Extra Features:\n",
        "\n",
        "# If available, includes max BMI and vaccine dose count.\n",
        "\n",
        "# 5. Aggregate Data by Patient:\n",
        "\n",
        "# Groups by person_id and applies aggregation.\n",
        "\n",
        "# 6. Join Death Data:\n",
        "\n",
        "# Merges patient death indicator.\n",
        "\n",
        "# 7. Filter Adult Females:\n",
        "\n",
        "# Restricts dataset to female patients aged 18+.\n",
        "\n",
        "\n",
        "\n",
        "def ALL_PATIENTS_SUMMARY(All_PATIENTS_FLAG,everyone_cohort_de_id,person, everyone_patient_deaths):\n",
        "\n",
        "    df = All_PATIENTS_FLAG\n",
        "    deaths_df = everyone_patient_deaths.select('person_id', 'patient_death')\n",
        "\n",
        "\n",
        "# Group all *_flag_1y_* and *_flag_280d_* one hot encoded columns\n",
        "\n",
        "    flag_groups = defaultdict(list)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if \"_flag_1y_\" in col:\n",
        "            dv = col.split(\"_flag_1y_\")[0]\n",
        "            time = col.split(\"_flag_1y_\")[1]\n",
        "            flag_groups[f\"{dv}_flag_1y\"].append((col, time))\n",
        "        elif \"_flag_280d_\" in col:\n",
        "            dv = col.split(\"_flag_280d_\")[0]\n",
        "            time = col.split(\"_flag_280d_\")[1]\n",
        "            flag_groups[f\"{dv}_flag_280d\"].append((col, time))\n",
        "\n",
        "\n",
        "# Aggregate one hot encoding columns to create *_indicator columns\n",
        "\n",
        "    agg_expressions = []\n",
        "\n",
        "    for dv_flag, cols in flag_groups.items():\n",
        "        for col_name, timing in cols:\n",
        "            suffix = f\"{dv_flag.replace('_flag_', '_')}_{timing}_indicator\"\n",
        "            agg_expressions.append(F.max(col_name).alias(suffix))\n",
        "\n",
        "\n",
        "# Additional columns (BMI, vaccine)\n",
        "\n",
        "    if \"BMI_rounded\" in df.columns:\n",
        "        agg_expressions.append(F.max(\"BMI_rounded\").alias(\"BMI_max_observed_or_calculated\"))\n",
        "    if \"had_vaccine_administered\" in df.columns:\n",
        "        agg_expressions.append(F.sum(\"had_vaccine_administered\").alias(\"total_number_of_COVID_vaccine_doses\"))\n",
        "\n",
        "\n",
        "# Run final aggregation\n",
        "\n",
        "    df = df.groupby(\"person_id\").agg(*agg_expressions)\n",
        "\n",
        "\n",
        "# Join death info\n",
        "\n",
        "    df = df.join(deaths_df, 'person_id', 'left').withColumnRenamed('patient_death', 'patient_death_indicator')\n",
        "\n",
        "\n",
        "# Join to cohort base and restrict to adults\n",
        "\n",
        "    df = everyone_cohort_de_id.join(df, 'person_id', 'right')\n",
        "    df = df.filter((F.col(\"age\") >= 18) & (F.col(\"sex\") == \"FEMALE\"))\n",
        "\n",
        "\n",
        "# Print unique patient count\n",
        "\n",
        "    unique_patient_count = df.select(\"person_id\").distinct().count()\n",
        "    print(f\"Number of unique patients: {unique_patient_count}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Global imports and functions included below\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
        "from collections import defaultdict\n",
        "from pyspark.sql.types import IntegerType, LongType, DoubleType\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rPSpkpDQGyia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final table grouping the columns and so on"
      ],
      "metadata": {
        "id": "yzhH23J0ZXQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: Generate a cleaned and finalized patient history summary table focused on delivery-related outcomes and comorbidities.\n",
        "#\n",
        "# Creator: [Team1]\n",
        "# Last Updated: [2025-04-20]\n",
        "#\n",
        "# Description:\n",
        "#   This function takes the `ALL_PATIENTS_SUMMARY` DataFrame and selects relevant demographic and clinical features\n",
        "#   including depression, preeclampsia, C-section history, and COVID diagnosis indicators. It creates flags for\n",
        "#   postpartum outcomes while excluding patients with a history of the condition prior to their first delivery\n",
        "#   (based on 1-year and 280-day time windows). This helps isolate incident (new) cases occurring postpartum.\n",
        "#\n",
        "#   The returned DataFrame is suitable for downstream analysis such as modeling or statistical comparisons.\n",
        "\n",
        "\n",
        "def Final_history_table(ALL_PATIENTS_SUMMARY):\n",
        "    df = ALL_PATIENTS_SUMMARY\n",
        "\n",
        "    df_selected = df.select(\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        # Depression\n",
        "        F.col(\"depression_1y_during_1y_postpartum_indicator\").alias(\"postpartum_depression\"),\n",
        "        F.col(\"depression_1y_during_1y_prior_to_1st_delivery_indicator\").alias(\"depression_hist_1y_prior\"),\n",
        "        F.col(\"depression_280d_during_280d_prior_to_1st_delivery_indicator\").alias(\"depression_hist_280d_prior\"),\n",
        "\n",
        "        # Preeclampsia\n",
        "        ## F.col(\"preeclampsia_1y_during_1y_postpartum_indicator\").alias(\"postpartum_preeclampsia\"), ## Not need it\n",
        "        F.col(\"preeclampsia_1y_during_1y_prior_to_1st_delivery_indicator\").alias(\"preeclampsia_hist_1y_prior\"),\n",
        "        F.col(\"preeclampsia_280d_during_280d_prior_to_1st_delivery_indicator\").alias(\"preeclampsia_hist_280d_prior\"),\n",
        "\n",
        "        # C-section # Like depression\n",
        "        F.col(\"delivery_mode_caesarean_1y_during_1y_postpartum_indicator\").alias(\"postpartum_csection\"),\n",
        "        F.col(\"delivery_mode_caesarean_1y_during_1y_prior_to_1st_delivery_indicator\").alias(\"csection_hist_1y_prior\"),\n",
        "        F.col(\"delivery_mode_caesarean_280d_during_280d_prior_to_1st_delivery_indicator\").alias(\"csection_hist_280d_prior\"),\n",
        "\n",
        "        # Covid diagnosis indicator\n",
        "        F.col(\"LL_COVID_diagnosis_1y_during_1y_prior_to_1st_delivery_indicator\").alias(\"covid_diag_hist_1y_prior\"),\n",
        "        F.col(\"LL_COVID_diagnosis_280d_during_280d_prior_to_1st_delivery_indicator\").alias(\"covid_diag_hist_280d_prior\")\n",
        "\n",
        "    )\n",
        "\n",
        "    # Create postpartum_depression_main: NA if history of prior depression, else keep postpartum_depression\n",
        "    df_selected = df_selected.withColumn(\n",
        "        \"postpartum_depression_main\",\n",
        "        F.when(F.col(\"depression_hist_1y_prior\") == 1, F.lit(None)).otherwise(F.col(\"postpartum_depression\"))\n",
        "    )\n",
        "\n",
        "    df_selected = df_selected.withColumn(\n",
        "        \"postpartum_csection_main\",\n",
        "        F.when(F.col(\"csection_hist_1y_prior\") == 1, F.lit(None)).otherwise(F.col(\"postpartum_csection\"))\n",
        "    )\n",
        "\n",
        "    df_selected = df_selected.withColumn(\n",
        "        \"postpartum_depression_sens\",\n",
        "        F.when(F.col(\"depression_hist_280d_prior\") == 1, F.lit(None)).otherwise(F.col(\"postpartum_depression\"))\n",
        "    )\n",
        "\n",
        "    df_selected = df_selected.withColumn(\n",
        "        \"postpartum_csection_sens\",\n",
        "        F.when(F.col(\"csection_hist_280d_prior\") == 1, F.lit(None)).otherwise(F.col(\"postpartum_csection\"))\n",
        "    )\n",
        "\n",
        "    return df_selected\n",
        "\n",
        "\n",
        "\n",
        "# Global imports and functions included below\n",
        "\n",
        "from pyspark.sql import functions as F\n"
      ],
      "metadata": {
        "id": "Nd6oxw4mZR15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding the race"
      ],
      "metadata": {
        "id": "zL113eXRRF5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "def feature_engineering(Final_history_table):\n",
        "    # Step 1: Select necessary columns\n",
        "    selected_columns = [\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\"\n",
        "    ]\n",
        "    df = Final_history_table.select(*selected_columns)\n",
        "\n",
        "    # Step 2: Replace value in the race column\n",
        "    replace_dict = {\n",
        "    'Black or African American': 'Black_or_African_American',\n",
        "    'Hispanic or Latino': 'Other',\n",
        "    'Native Hawaiian or Other Pacific Islander': 'Other',\n",
        "    'American Indian or Alaska Native': 'Other'\n",
        "    }\n",
        "\n",
        "    df = df.replace(replace_dict, subset=[\"race\"])\n",
        "\n",
        "    # Step 3: One-hot encode 'sex' column\n",
        "    gender_categories = df.select('sex').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    gender_exprs = [F.when(F.col('sex') == cat, 1).otherwise(0).alias(f\"sex_{cat}\") for cat in gender_categories]\n",
        "\n",
        "    # Step 4: One-hot encode 'race' column\n",
        "    race_categories = df.select('race').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    race_exprs = [F.when(F.col('race') == cat, 1).otherwise(0).alias(f\"race_{cat}\") for cat in race_categories]\n",
        "\n",
        "    # Step 5: Combine original features with one-hot encoded columns\n",
        "    df = df.select(*df.columns, *gender_exprs, *race_exprs)\n",
        "\n",
        "    # Step 6: Drop original categorical columns and optional categories (to avoid multicollinearity)\n",
        "    cols_to_drop = [\"race\"]\n",
        "    optional_drop = [\"race_Unknown\"]  # Drop one value per set for dummy variable trap\n",
        "    for col in optional_drop:\n",
        "        if col in df.columns:\n",
        "            cols_to_drop.append(col)\n",
        "    df = df.drop(*cols_to_drop)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "AWJ0PEoLQ4PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the Indicators that were taken into account"
      ],
      "metadata": {
        "id": "Mj6XfflVTvSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "        #Baseline\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\"\n",
        "\n",
        "\n",
        "        #Other indicators\n",
        "        \"ectopic_pregnancy_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"chronic_lung_disease_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"eclampsia_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"eclampsia_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"eclampsia_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"eclampsia_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"preeclampsia_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"preeclampsia_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"preeclampsia_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"preeclampsia_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"psychosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"OBESITY_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"total_number_of_COVID_vaccine_doses\","
      ],
      "metadata": {
        "id": "amw17vXzQ4M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the simple statistics of the groups to do descriptive analysis statistcs: in pythion and R code for the comparison\n"
      ],
      "metadata": {
        "id": "Khw68y5CC0vQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paired t test"
      ],
      "metadata": {
        "id": "TZWSmmjtC6a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The code performs a paired Ttest to know if there's a significant difference in systolic blood pressure before and after 6 weeks of treatment with Drug X in patients with hypertension\"\n",
        "# Looking at postpartum_year after depression and History 280 days Before\n",
        "# This is R code\n",
        "\n",
        "# Define paired t-test function\n",
        "Paired_t_TestPostpartum_280DaysBefore <- function(Final_history_table) {\n",
        "\n",
        "  # Perform the paired samples t-test\n",
        "  t_test_result <- t.test(Final_history_table$postpartum_depression, Final_history_table$depression_hist_280d_prior , paired = TRUE)\n",
        "\n",
        "   # Print the result of the t-test\n",
        "  print(t_test_result)\n",
        "\n",
        "   # Return the t-test result for further use\n",
        "  return(NULL)\n",
        "\n",
        "}\n",
        "\n",
        "#RESULT INTERPRETATION\n",
        "# T test 92 \tThis is the t-statistic. A high number = a strong difference between the two groups.\n",
        "# \tOn average, postpartum depression is 3.92% higher than depression during pregnancy.\n",
        "# This is a very significant result.\n",
        "\n",
        "#  following the idea:\n",
        "# You have two groups of weights.\n",
        "\n",
        "# Within each group, the weights are clustered tightly (small standard deviation).\n",
        "\n",
        "# But the average weight between the two groups is quite different.\n",
        "\n",
        "#RESULT: Because the groups are internally consistent (not spread out)...\n",
        "\n",
        "# And the difference between their means is big...\n",
        "\n",
        "# That gives us a large t-statistic → which means the difference is statistically significant!\n",
        "\n",
        "\n",
        "\n",
        "#IN Class Example:\n",
        "\n",
        "#Key Components of the Output:\n",
        "\n",
        "#t-value:\n",
        "\n",
        "#t = -41.468: This is the calculated test statistic, which is quite large in absolute value. The negative sign indicates that the After values are smaller than the Before values on average. This suggests a strong decrease in whatever measurement you're testing (e.g., weight, blood pressure, etc.).\n",
        "\n",
        "#Degrees of Freedom (df):\n",
        "\n",
        "#df = 299: The degrees of freedom for the test, which is calculated as n - 1, where n is the number of pairs. Since you have 300 pairs of data, the degrees of freedom is 299.\n",
        "\n",
        "#p-value:\n",
        "\n",
        "#p-value < 2.2e-16: This is extremely small, much less than the typical significance level of 0.05. This means there is a very strong statistical significance, and we can confidently reject the null hypothesis (which states that the mean difference between Before and After is 0). Therefore, there is a significant difference between the Before and After measurements.\n",
        "\n",
        "#Alternative Hypothesis:\n",
        "\n",
        "#The alternative hypothesis being tested is that the true mean difference between the Before and After measurements is not equal to 0 (i.e., the two means are different).\n",
        "\n",
        "#95% Confidence Interval:\n",
        "\n",
        "#Confidence interval: The 95% confidence interval for the mean difference is between -53.62 and -48.76. Since the entire interval is negative and does not contain 0, this further supports the conclusion that the After measurements are significantly smaller than the Before measurements.\n",
        "\n",
        "#Mean Difference:\n",
        "\n",
        "#mean difference = -51.19333: The average difference between the Before and After measurements is about -51.19. This indicates that, on average, the value of After is 51.19 units smaller than Before. This suggests a substantial decrease in the measurements from Before to After.\n",
        "\n",
        "#Interpretation:\n",
        "#The large t-value (-41.468) and very small p-value (< 2.2e-16) suggest a highly significant result, indicating that the difference between the two groups (Before and After) is statistically significant.\n",
        "\n",
        "#The mean difference of -51.19 suggests that whatever variable you’re measuring (e.g., weight, blood pressure, etc.) has decreased on average by about 51.19 units after the intervention or treatment.\n",
        "\n",
        "#The 95% confidence interval for the mean difference is (-53.62, -48.76), which tells you that the true mean difference between the Before and After measurements lies within this range with 95% confidence. Since the entire confidence interval is negative and does not include 0, it further supports the conclusion of a significant decrease.\n",
        "\n",
        "#Conclusion:\n",
        "#Based on the results of the paired t-test:\n",
        "\n",
        "#There is a statistically significant decrease in the measurement from Before to After.\n",
        "\n",
        "#The mean difference is approximately -51.19, meaning the After measurement is lower than the Before measurement by that amount.\n",
        "\n",
        "#The p-value is extremely small (less than 2.2e-16), indicating very strong evidence against the null hypothesis that there is no difference between the two time points.\n"
      ],
      "metadata": {
        "id": "S5nA7DmLQ4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ttest python\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def Python_age_ttest(Final_history_table):\n",
        "\n",
        "    \"\"\"\n",
        "    Perform a one-sample t-test (greater) comparing the mean of 'age' to 35 using Spark DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    Final (DataFrame): A PySpark DataFrame with an 'age' column.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    final_pd = Final_history_table.select(\"age\").toPandas()\n",
        "\n",
        "    # Perform the one-sample t-test\n",
        "    sample = final_pd['age']\n",
        "    t_statistic, p_value_two_sided = stats.ttest_1samp(sample, popmean=35)\n",
        "\n",
        "    # Convert two-sided p-value to one-sided (greater)\n",
        "    if t_statistic > 0:\n",
        "        p_value_one_sided = p_value_two_sided / 2\n",
        "    else:\n",
        "        p_value_one_sided = 1 - (p_value_two_sided / 2)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"T-statistic:\", t_statistic)\n",
        "    print(\"One-sided p-value (greater):\", p_value_one_sided)\n"
      ],
      "metadata": {
        "id": "4Jf7MYnDQ4HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R t-test\n",
        "RprogamT_test_Age35 <- function(Final_history_table) {\n",
        "\n",
        "    #the function compares the mean of age to 35.\n",
        "  t_test_result <- t.test(Final_history_table$age, mu = 35, alternative=\"greater\")\n",
        "\n",
        "\n",
        "  # Print the result of the t-test\n",
        "  print(t_test_result)\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "jQbcbXS5TIZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paied t test python\n",
        "# Working on this one\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from scipy.stats import t\n",
        "\n",
        "def paired_t_test_postpartum_1y(Final_history_table):\n",
        "    \"\"\"\n",
        "    Perform a paired t-test in PySpark between 'postpartum_depression' and\n",
        "    'depression_hist_1y_prior' without converting to Pandas.\n",
        "\n",
        "    Parameters:\n",
        "    - Final_history_table: PySpark DataFrame\n",
        "\n",
        "    Prints:\n",
        "    - T-statistic, sample size, mean difference, and p-value approximation\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Select and drop missing data\n",
        "    df_filtered = Final_history_table.select(\n",
        "        \"postpartum_depression\",\n",
        "        \"depression_hist_1y_prior\"\n",
        "    ).dropna(subset=[\"postpartum_depression\", \"depression_hist_1y_prior\"])\n",
        "\n",
        "    # Step 2: Compute difference\n",
        "    df_diff = df_filtered.withColumn(\n",
        "        \"diff\",\n",
        "        F.col(\"postpartum_depression\") - F.col(\"depression_hist_1y_prior\")\n",
        "    )\n",
        "\n",
        "    # Step 3: Calculate statistics\n",
        "    stats = df_diff.select(\n",
        "        F.count(\"diff\").alias(\"n\"),\n",
        "        F.mean(\"diff\").alias(\"mean_diff\"),\n",
        "        F.stddev(\"diff\").alias(\"std_diff\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    n = stats[\"n\"]\n",
        "    mean_diff = stats[\"mean_diff\"]\n",
        "    std_diff = stats[\"std_diff\"]\n",
        "\n",
        "    # Step 4: Calculate t-statistic\n",
        "    t_stat = mean_diff / (std_diff / (n ** 0.5)) if std_diff else float('inf')\n",
        "\n",
        "    # Step 5: Approximate two-sided p-value\n",
        "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n-1))\n",
        "\n",
        "    # Output\n",
        "    print(\"\\nPaired t-test between postpartum_depression and depression_hist_1y_prior:\")\n",
        "    print(f\"Sample size (n): {n}\")\n",
        "    print(f\"Mean difference: {mean_diff:.8f}\")\n",
        "    print(f\"Standard deviation of differences: {std_diff:.8f}\")\n",
        "    print(f\"T-statistic: {t_stat:.3f}\")\n",
        "    print(f\"Approx. P-value (two-sided): {p_value:.3g}\")\n"
      ],
      "metadata": {
        "id": "ChA4QkOlTIW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#The code performs a paired Ttest to know if there's a significant difference in systolic blood pressure before and after 6 weeks of treatment with Drug X in patients with hypertension\"\n",
        "# Looking at postpartum_year after depression and Historry Year Before\n",
        "\n",
        "# Define paired t-test function\n",
        "Paired_t_TestPostpartum_1YearBefore <- function(Final_history_table) {\n",
        "\n",
        "  # Perform the paired samples t-test\n",
        "  t_test_result <- t.test(Final_history_table$postpartum_depression, Final_history_table$depression_hist_1y_prior, paired = TRUE)\n",
        "\n",
        "   # Print the result of the t-test\n",
        "  print(t_test_result)\n",
        "\n",
        "   # Return the t-test result for further use\n",
        "  return(NULL)\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "a-y8R5O5TISQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi test\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "\n",
        "def chi_square_postpartum(Final_history_table):\n",
        "\n",
        "    # Count occurrences of each value in postpartum_depression (assumed to be binary: 0/1)\n",
        "    counts_df = Final_history_table.groupBy(\"postpartum_depression\").count()\n",
        "\n",
        "    # Collect the results to the driver\n",
        "    counts = counts_df.orderBy(\"postpartum_depression\").select(\"count\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    # Check if there are exactly 2 categories (0 and 1)\n",
        "    if len(counts) != 2:\n",
        "        print(\"There must be exactly two categories (e.g., 0 and 1) in 'postpartum_depression'.\")\n",
        "        return\n",
        "\n",
        "    # Expected frequencies for a 50/50 distribution\n",
        "    total = sum(counts)\n",
        "    expected = [total / 2, total / 2]\n",
        "\n",
        "    # Run Chi-Square Goodness-of-Fit test\n",
        "    chi_stat, p_value = chisquare(counts, f_exp=expected)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Observed counts (postpartum_depression): {counts}\")\n",
        "    print(f\"Expected counts (50/50): {expected}\")\n",
        "    print(\"Chi-square statistic:\", chi_stat)\n",
        "    print(\"P-value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "KODB_QNGTIPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chi test r\n",
        "# Results:\n",
        "# The number of people with and without postpartum depression is not close to 50:50. It's very unbalanced, and that difference is statistically significant.\n",
        "\n",
        "\n",
        "# Define the Chi-Square Goodness of Fit function with a 50:50 expected distribution\n",
        "Chi_Square_Goodness_50_50_R <- function(Final_history_table) {\n",
        "\n",
        "  # Check if the 'postpartum_depression' variable exists in the dataset\n",
        "  if (!\"postpartum_depression\" %in% colnames(Final_history_table)) {\n",
        "    stop(\"The 'postpartum_depression' variable does not exist in the dataset.\")\n",
        "  }\n",
        "\n",
        "  # Creating observed frequencies for 'postpartum_depression' in the dataset\n",
        "  observed <- table(Final_history_table$postpartum_depression)  # Count occurrences of each level\n",
        "\n",
        "  # Ensure the observed data has exactly two categories (e.g., 0 and 1)\n",
        "  if (length(observed) != 2) {\n",
        "    stop(\"There must be exactly two levels in the 'postpartum_depression' variable.\")\n",
        "  }\n",
        "\n",
        "  # Expected probabilities for a 50:50 distribution\n",
        "  expected_probabilities <- c(0.5, 0.5)\n",
        "\n",
        "  # Perform the Chi-Square Goodness of Fit Test\n",
        "  chi_square_result <- chisq.test(observed, p = expected_probabilities)\n",
        "\n",
        "  # Print the result of the Chi-Square test\n",
        "  print(chi_square_result)\n",
        "\n",
        "  # Return the Chi-Square result for further use\n",
        "  return(Final_history_table)\n",
        "}\n"
      ],
      "metadata": {
        "id": "GGQACYZoQ4Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mann Wihtney R\n",
        "#The code performs a two-sample-T-test comparing the mean of weight between participants using the Wilcoxon rank-sum test (also known as Mann-Whitney U test) (This is a Non-Parametric Tests)\n",
        "\n",
        "# Define Mann_Whitneyfunction\n",
        "Mann_Whitney_Test <- function(Final_history_table) {\n",
        "\n",
        "# Perform the Mann-Whitney U test (Wilcoxon rank sum test)\n",
        "    mann_whitney_test_result <- wilcox.test(age ~ postpartum_depression, data = Final_history_table)\n",
        "\n",
        " # Print the test result\n",
        "     print(mann_whitney_test_result)\n",
        "\n",
        "# Return the test result\n",
        "     return(Final_history_table)\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "Lr4GYXk1X7PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Featrue Engineering"
      ],
      "metadata": {
        "id": "bCKbkOHc4wRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "def Feature_Eng_5( unnamed_7):\n",
        "    # Step 1: Select necessary columns\n",
        "    selected_columns = [\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\",\n",
        "        #New Indicators\n",
        "        \"eclampsia_any_1y_prior_to_1st_delivery\",\"eclampsia_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"eclampsia_any_280d_before_prior_to_1st_delivery\",\"eclampsia_any_280d_during_prior_to_1st_delivery\",\n",
        "\n",
        "        \"ectopic_pregnancy_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"chronic_lung_disease_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "\n",
        "        \"hypertension_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "\n",
        "        \"psychosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"psychosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_1y_postpartum_indicator\",\n",
        "        #New Indicators\n",
        "        \"diabetes_any_1y_prior_to_1st_delivery\",\"diabetes_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"diabetes_any_280d_before_prior_to_1st_delivery\",\"diabetes_any_280d_during_prior_to_1st_delivery\",\n",
        "\n",
        "        \"OBESITY_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"total_number_of_COVID_vaccine_doses\"\n",
        "\n",
        "    ]\n",
        "    df = unnamed_7.select(*selected_columns)\n",
        "\n",
        "    # Step 2: Replace value in the race column\n",
        "    replace_dict = {\n",
        "    'Black or African American': 'Black_or_African_American',\n",
        "    'Hispanic or Latino': 'Other',\n",
        "    'Native Hawaiian or Other Pacific Islander': 'Other',\n",
        "    'American Indian or Alaska Native': 'Other'\n",
        "    }\n",
        "\n",
        "    df = df.replace(replace_dict, subset=[\"race\"])\n",
        "\n",
        "    # Step 3: One-hot encode 'sex' column\n",
        "    gender_categories = df.select('sex').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    gender_exprs = [F.when(F.col('sex') == cat, 1).otherwise(0).alias(f\"sex_{cat}\") for cat in gender_categories]\n",
        "\n",
        "    # Step 4: One-hot encode 'race' column\n",
        "    race_categories = df.select('race').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    race_exprs = [F.when(F.col('race') == cat, 1).otherwise(0).alias(f\"race_{cat}\") for cat in race_categories]\n",
        "\n",
        "    # Step 5: Combine original features with one-hot encoded columns\n",
        "    df = df.select(*df.columns, *gender_exprs, *race_exprs)\n",
        "\n",
        "    # Step 6: Drop original categorical columns and optional categories (to avoid multicollinearity)\n",
        "    cols_to_drop = [\"race\"]\n",
        "    optional_drop = [\"race_Unknown\"]  # Drop one value per set for dummy variable trap\n",
        "    for col in optional_drop:\n",
        "        if col in df.columns:\n",
        "            cols_to_drop.append(col)\n",
        "    df = df.drop(*cols_to_drop)\n",
        "\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "vuCSYgrzX7Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code it to try the coding of one hot encodding and it is on the old node that has only selected columns\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def feature_engineering2(Final_history_table):\n",
        "    # Step 1: Select necessary columns\n",
        "    selected_columns = [\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\"\n",
        "    ]\n",
        "    df = Final_history_table.select(*selected_columns)\n",
        "\n",
        "    # Step 2: Replace value in the race column\n",
        "    replace_dict = {\n",
        "        'Black or African American': 'Black_or_African_American',\n",
        "        'Hispanic or Latino': 'Other',\n",
        "        'Native Hawaiian or Other Pacific Islander': 'Other',\n",
        "        'American Indian or Alaska Native': 'Other'\n",
        "    }\n",
        "    df = df.replace(replace_dict, subset=[\"race\"])\n",
        "\n",
        "    # Step 3: One-hot encode 'race' column (only)\n",
        "    race_categories = df.select('race').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    race_exprs = [F.when(F.col('race') == cat, 1).otherwise(0).alias(f\"race_{cat}\") for cat in race_categories]\n",
        "\n",
        "    # Step 4: Combine original features with one-hot encoded columns\n",
        "    df = df.select(*df.columns, *race_exprs)\n",
        "\n",
        "    # Step 5: Drop original categorical columns and optional categories (to avoid multicollinearity)\n",
        "    cols_to_drop = [\"race\"]\n",
        "    optional_drop = [\"race_Unknown\"]  # Drop one value per set for dummy variable trap\n",
        "    for col in optional_drop:\n",
        "        if col in df.columns:\n",
        "            cols_to_drop.append(col)\n",
        "    df = df.drop(*cols_to_drop)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "tVyY0GgLX7IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byvFHWyf0s8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing gender as 1  for the FEMALe"
      ],
      "metadata": {
        "id": "qV3ecTCb0rXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def Feature_Eng_1(Data_cleaning_1):\n",
        "\n",
        "    # Step 1: Keep all columns\n",
        "    df = Data_cleaning_1\n",
        "\n",
        "    # Step 2: Replace values in the 'race' column\n",
        "    replace_dict = {\n",
        "        'Black or African American': 'Black_or_African_American',\n",
        "        'Hispanic or Latino': 'Other',\n",
        "        'Native Hawaiian or Other Pacific Islander': 'Other',\n",
        "        'American Indian or Alaska Native': 'Other'\n",
        "    }\n",
        "\n",
        "\n",
        "    # Step 2.5: Replace 'FEMALE' in 'sex' column with 1\n",
        "    df = df.withColumn('sex', F.when(F.col('sex') == 'FEMALE', 1).otherwise(F.col('sex')))\n",
        "\n",
        "    # Step 3: One-hot encode 'race' column (only)\n",
        "    df = df.replace(replace_dict, subset=[\"race\"])\n",
        "\n",
        "    # Step 3: One-hot encode 'race' column (only)\n",
        "    race_categories = df.select('race').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    race_exprs = [F.when(F.col('race') == cat, 1).otherwise(0).alias(f\"race_{cat}\") for cat in race_categories]\n",
        "\n",
        "    # Step 4: Combine original features with one-hot encoded columns\n",
        "    df = df.select(*df.columns, *race_exprs)\n",
        "\n",
        "    # Step 5: Drop original categorical columns and optional categories (to avoid multicollinearity)\n",
        "    cols_to_drop = [\"race\"]\n",
        "    optional_drop = [\"race_Unknown\"]  # Drop one value per set for variable trap\n",
        "    for col in optional_drop:\n",
        "        if col in df.columns:\n",
        "            cols_to_drop.append(col)\n",
        "    df = df.drop(*cols_to_drop)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "FHrP1t_R0qAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for data cleaning to focuse on age 18 till 50, and gender"
      ],
      "metadata": {
        "id": "rT379nYOIuyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: Prepares and transforms patient data by selecting relevant clinical and demographic features,\n",
        "# standardizing the \"race\" column, one-hot encoding it (done in a previous node),\n",
        "# and filtering the cohort to include only patients aged 18 to 50.\n",
        "# Team1_\n",
        "# Date: 04_24_25\n",
        "# Description: Performs feature engineering on a Spark DataFrame by filtering valid age and sex values,\n",
        "# selecting specific races, and restricting the cohort to patients aged 18 to 50.\n",
        "\n",
        "def Data_Cleaning_6(Feature_Eng_5):\n",
        "\n",
        "    selected_columns = [\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\",\n",
        "        # New Indicators\n",
        "        \"eclampsia_any_1y_prior_to_1st_delivery\",\"eclampsia_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"eclampsia_any_280d_before_prior_to_1st_delivery\",\"eclampsia_any_280d_during_prior_to_1st_delivery\",\n",
        "        \"ectopic_pregnancy_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"chronic_lung_disease_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"psychosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"diabetes_any_1y_prior_to_1st_delivery\",\"diabetes_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"diabetes_any_280d_before_prior_to_1st_delivery\",\"diabetes_any_280d_during_prior_to_1st_delivery\",\n",
        "        \"OBESITY_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"total_number_of_COVID_vaccine_doses\"\n",
        "    ]\n",
        "\n",
        "    # Filter for valid age between 18 and 50\n",
        "    df_age = Feature_Eng_5.filter((Feature_Eng_5.age >= 18) & (Feature_Eng_5.age <= 50))\n",
        "    print('# of patients aged 18 to 50:', df_age.count())\n",
        "\n",
        "    # Filter for valid gender\n",
        "    df_gender = df_age.filter((df_age.sex == 'MALE') | (df_age.sex == 'FEMALE'))\n",
        "    print('# of patients with gender Male or Female:', df_gender.count())\n",
        "\n",
        "    # Filter for selected race categories\n",
        "    df_race = df_gender.filter(\n",
        "        (df_gender.race == 'White') |\n",
        "        (df_gender.race == 'Asian') |\n",
        "        (df_gender.race == 'Black or African American') |\n",
        "        (df_gender.race == 'Unknown')\n",
        "    )\n",
        "    print('# of patients with selected race categories:', df_race.count())\n",
        "\n",
        "    return df_race\n"
      ],
      "metadata": {
        "id": "PEY_I6chX6zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: Prepares and transforms patient data by selecting relevant clinical and demographic features, standardizing the \"race\" column, one-hot encoding it that was done in the previous node, and filtering the cohort to include only patients aged 18 to 50.\n",
        "# Team1_\n",
        "# Date: 04_24_25\n",
        "# Description: It performs feature engineering on a Spark DataFrame by Filtering the cohort to include only patients aged 18 to 50.\n",
        "\n",
        "\n",
        "# Purpose: Prepares and transforms patient data by selecting relevant clinical and demographic features,\n",
        "# and filtering the cohort to include only patients aged 18 to 50.\n",
        "# Team1_\n",
        "# Date: 04_24_25\n",
        "# Description: Performs feature engineering on a Spark DataFrame by filtering on age\n",
        "# and selecting specific clinical and demographic columns.\n",
        "\n",
        "def Data_Cleaning_6(Feature_Eng_5):\n",
        "\n",
        "    selected_columns = [\n",
        "        \"person_id\", \"total_visits\", \"sex\", \"age\", \"race\", \"race_ethnicity\", \"data_partner_id\",\n",
        "        \"postpartum_depression\", \"depression_hist_1y_prior\", \"depression_hist_280d_prior\",\n",
        "        \"preeclampsia_hist_1y_prior\", \"preeclampsia_hist_280d_prior\", \"postpartum_csection\",\n",
        "        \"csection_hist_1y_prior\", \"csection_hist_280d_prior\", \"covid_diag_hist_1y_prior\",\n",
        "        \"covid_diag_hist_280d_prior\", \"postpartum_depression_main\", \"postpartum_csection_main\",\n",
        "        \"postpartum_depression_sens\", \"postpartum_csection_sens\",\n",
        "        \"eclampsia_any_1y_prior_to_1st_delivery\",\"eclampsia_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"eclampsia_any_280d_before_prior_to_1st_delivery\",\"eclampsia_any_280d_during_prior_to_1st_delivery\",\n",
        "        \"ectopic_pregnancy_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"tobacco_smoker_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"LL_COVID_diagnosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"chronic_lung_disease_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"chronic_lung_disease_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"hypertension_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"maternal_obesity_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_1y_during_1y_postpartum_indicator\",\n",
        "        \"psychosis_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"psychosis_280d_during_1y_postpartum_indicator\",\n",
        "        \"diabetes_any_1y_prior_to_1st_delivery\",\"diabetes_any_before__1y_prior_to_1st_delivery\",\n",
        "        \"diabetes_any_280d_before_prior_to_1st_delivery\",\"diabetes_any_280d_during_prior_to_1st_delivery\",\n",
        "        \"OBESITY_1y_during_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_1y_before_1y_prior_to_1st_delivery_indicator\",\n",
        "        \"OBESITY_280d_before_280d_prior_to_1st_delivery_indicator\",\n",
        "        \"total_number_of_COVID_vaccine_doses\"\n",
        "    ]\n",
        "\n",
        "    # Filter for valid age between 18 and 50\n",
        "    df_age = Feature_Eng_5.filter((Feature_Eng_5.age >= 18) & (Feature_Eng_5.age <= 50))\n",
        "    print('# of patients aged 18 to 50:', df_age.count())\n",
        "\n",
        "    # Select only the relevant columns\n",
        "    df_selected = df_age.select(*selected_columns)\n",
        "\n",
        "    return df_selected\n"
      ],
      "metadata": {
        "id": "_HfLtb-oIucB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "FCcuGtVjayX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose: Feature Scaling allowed us to scale the continous value age and applies standard scaling.  Then, it concatenates the scaled continuous features back with the categorical features. Finally, it removes quasi-constant features, which have 99% of their #values the same, to ensure the dataset is suitable for model building.\n",
        "# Date: 04_25_25\n",
        "# Description: Addoption of the ML_demo_Python code\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def Feature_scaling_1(Feature_Eng_1):\n",
        "\n",
        "\n",
        "    df = Feature_Eng_1\n",
        "    continuous_features = [\"age\"]\n",
        "\n",
        "    # Step 1: Assemble features into a vector\n",
        "    assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"cont_features_vec\")\n",
        "    df_vector = assembler.transform(df)\n",
        "\n",
        "    # Step 2: Apply standard scaler\n",
        "    scaler = StandardScaler(inputCol=\"cont_features_vec\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "    scaler_model = scaler.fit(df_vector)\n",
        "    df_scaled = scaler_model.transform(df_vector)\n",
        "\n",
        "    # Step 3: Extract scaled features back to columns (if needed)\n",
        "    # In Spark, the scaled data stays in vector form. If you want to split it into columns, use this:\n",
        "    from pyspark.ml.functions import vector_to_array\n",
        "    df_scaled = df_scaled.withColumn(\"scaled_age\", vector_to_array(\"scaled_features\")[0])\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    df_final = df_scaled.drop(\"cont_features_vec\", \"scaled_features\")\n",
        "\n",
        "    return df_final\n",
        "\n"
      ],
      "metadata": {
        "id": "XjRtghmWIuXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def feature_scaling_2_pyspark(Feature_Eng_1):\n",
        "    \"\"\"\n",
        "    Scales the 'age' column and replaces it with a new 'age_scaled' column in a PySpark DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        Feature_Eng_1 (pyspark.sql.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: DataFrame with 'age_scaled' instead of 'age'.\n",
        "    \"\"\"\n",
        "    df = Feature_Eng_1\n",
        "\n",
        "    # Step 1: Assemble 'age' into a feature vector\n",
        "    assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"age_unscaled_vec\")\n",
        "    df = assembler.transform(df)\n",
        "\n",
        "    # Step 2: Scale 'age' using StandardScaler\n",
        "    scaler = StandardScaler(inputCol=\"age_unscaled_vec\", outputCol=\"age_scaled_vec\", withMean=True, withStd=True)\n",
        "    scaler_model = scaler.fit(df)\n",
        "    df = scaler_model.transform(df)\n",
        "\n",
        "    # Step 3: Extract the scaled value from the vector into a regular float column\n",
        "    df = df.withColumn(\"age_scaled\", F.col(\"age_scaled_vec\")[0])\n",
        "\n",
        "    # Step 4: Drop intermediate columns ('age' and vector columns)\n",
        "    df = df.drop(\"age\", \"age_unscaled_vec\", \"age_scaled_vec\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "FiaCh0FLonb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, VarianceThresholdSelector\n",
        "\n",
        "def feature_scaling_2_pyspark(Feature_Eng_1):\n",
        "    \"\"\"\n",
        "    Scales continuous features and removes quasi-constant features from a PySpark DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        Feature_Eng_1 (pyspark.sql.DataFrame): Input DataFrame with features.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: Scaled and cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    df = Feature_Eng_1\n",
        "\n",
        "    # Continuous feature\n",
        "    continuous_features = [\"age\"]\n",
        "\n",
        "    # Assemble continuous features into a single vector column\n",
        "    assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"features_unscaled\")\n",
        "    df = assembler.transform(df)\n",
        "\n",
        "    # Apply StandardScaler\n",
        "    scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
        "    scaler_model = scaler.fit(df)\n",
        "    df = scaler_model.transform(df)\n",
        "\n",
        "    # Remove quasi-constant features (variance threshold)\n",
        "    selector = VarianceThresholdSelector(\n",
        "        varianceThreshold=0.01,\n",
        "        featuresCol=\"features_scaled\",\n",
        "        outputCol=\"features_selected\"\n",
        "    )\n",
        "    selector_model = selector.fit(df)\n",
        "    df = selector_model.transform(df)\n",
        "\n",
        "    # At this point, \"features_selected\" contains your cleaned features.\n",
        "    # If you want to split it back into columns, it’s a few more steps. Otherwise you can keep as vector.\n",
        "\n",
        "    return df.select(\"features_selected\")\n"
      ],
      "metadata": {
        "id": "jDPaarPHonY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the logistic regression example"
      ],
      "metadata": {
        "id": "JlxFe55abPNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code builds and evaluates a logistic regression model to predict mortality in patients. It first splits the data into training and testing sets. Then, it trains the #logistic regression model using the training data and evaluates it using the testing data. Various evaluation metrics such as accuracy, F1 score, precision, recall, and AUC-ROC are #calculated to assess the model's performance. A ROC curve is also plotted to visualize the model's performance\n",
        "# Logistic Regression Model for Mortality Prediction\n",
        "\n",
        "def logistic_regression_model(Feature_scaling_1):\n",
        "    df = feature_scaling\n",
        "    y = df['patient_death_indicator']  # Target variable\n",
        "    df.drop(['patient_death_indicator'], axis=1, inplace=True)  # Drop target variable from features\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_x, test_x, train_y, test_y = train_test_split(df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Print the shape of the training and testing sets\n",
        "    print(\"Train_x:\", train_x.shape)\n",
        "    print(\"Test_x:\", test_x.shape)\n",
        "    print(\"Train_y:\", train_y.shape)\n",
        "    print(\"Test_y:\", test_y.shape)\n",
        "\n",
        "    # Initialize and train the logistic regression model\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    logreg.fit(train_x, train_y)\n",
        "\n",
        "    # Evaluate the model's performance\n",
        "    print(\"Logistic Regression model Accuracy:\", logreg.score(test_x, test_y))\n",
        "    print(\"Logistic Regression F1 Score:\", f1_score(test_y, logreg.predict(test_x), average='macro'))\n",
        "    print(classification_report(test_y, logreg.predict(test_x)))\n",
        "\n",
        "    # Predict probabilities for the test set\n",
        "    logreg_pred_proba = logreg.predict_proba(test_x)\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, thresholds = roc_curve(test_y, logreg_pred_proba[:, 1])\n",
        "    auc = roc_auc_score(test_y, logreg_pred_proba[:, 1])\n",
        "\n",
        "    # Create the ROC curve plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot([0, 1], [0, 1], \"k--\")\n",
        "    plt.plot(fpr, tpr, label=\"AUC = {:.2f}\".format(auc))\n",
        "    plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
        "    plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
        "    plt.title(\"Logistic Regression ROC Curve\", fontsize=16)\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()\n",
        "\n",
        "    # Predict the labels for the test set\n",
        "    y_pred = logreg.predict(test_x)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print('Recall Score: {:0.3f}'.format(recall_score(test_y, y_pred, average='macro')))\n",
        "    print('Precision Score: {:0.3f}'.format(precision_score(test_y, y_pred, average='macro')))\n",
        "    print('F1 Score: {:0.3f}'.format(f1_score(test_y, y_pred, average='macro')))\n",
        "    print('Accuracy Score: {:0.3f}'.format(accuracy_score(test_y, y_pred)))\n",
        "    print('AUC score: {:0.3f}'.format(auc))\n",
        "    print('Training set:', train_x.shape[0])\n",
        "    print('Test set:', test_x.shape[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "dSxZqWuoIuSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EL7BlihnX6vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nut47VVbX6q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics codes:\n",
        "# Example\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "def unnamed_4(ALL_PATIENTS_SUMMARY):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Analyze age differences between racial groups from a PySpark DataFrame.\n",
        "    - Shows age statistics grouped by race.\n",
        "    - Performs a two-sample t-test on age between two selected races.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Spark DataFrame to Pandas\n",
        "    final_pd = ALL_PATIENTS_SUMMARY.select(\"age\", \"race_ethnicity\").toPandas()\n",
        "\n",
        "    # Display unique race values for reference\n",
        "    print(\"Available races:\", final_pd[\"race_ethnicity\"].unique())\n",
        "\n",
        "    # Group statistics by race\n",
        "    grouped = final_pd.groupby(\"race_ethnicity\")[\"age\"].describe()\n",
        "    print(\"\\nAge summary statistics by race_ethnicity:\\n\", grouped)\n",
        "\n",
        "    # Example two-group comparison (adjust race names to fit your data)\n",
        "    race1 = \"White Non-Hispanic\"\n",
        "    race2 = \"Hispanic or Latino Any Race\"\n",
        "\n",
        "    age_race1 = final_pd[final_pd[\"race_ethnicity\"] == race1][\"age\"]\n",
        "    age_race2 = final_pd[final_pd[\"race_ethnicity\"] == race2][\"age\"]\n",
        "\n",
        "    # Drop NaN values just in case\n",
        "    age_race1 = age_race1.dropna()\n",
        "    age_race2 = age_race2.dropna()\n",
        "\n",
        "    # Welch's t-test (assumes unequal variances)\n",
        "    t_stat, p_val = stats.ttest_ind(age_race1, age_race2, equal_var=False)\n",
        "\n",
        "    print(f\"\\nTwo-sample t-test between '{race1}' and '{race2}':\")\n",
        "    print(\"T-statistic:\", t_stat)\n",
        "    print(\"P-value:\", p_val)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-qHm2T3-Q3-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}